{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUe8amYRKYXH",
        "outputId": "2718683c-22a7-4f5d-e794-fa08ae0ef6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import torch\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "from collections import Counter\n",
        "\n",
        "# Read the DOCX file\n",
        "doc_path = \"wikipedia.docx\"\n",
        "doc = Document(doc_path)\n",
        "\n",
        "# Extract text from paragraphs\n",
        "text_data = [paragraph.text for paragraph in doc.paragraphs]\n",
        "\n",
        "\n",
        "# Remove special characters and words between them using regex\n",
        "text_data = [re.sub(r\"\\[.*?\\]\", \"\", text) for text in text_data]\n",
        "\n",
        "# Remove words not in the English alphabet\n",
        "english_alphabet = set(string.ascii_lowercase)\n",
        "text_data = [' '.join([word for word in text.split()\n",
        "                       if all(char in english_alphabet\n",
        "                              for char in word)]) for text in text_data]\n",
        "\n",
        "# Create a DataFrame with the cleaned text data\n",
        "df = pd.DataFrame({\"Text\": text_data})\n",
        "\n",
        "# Save the cleaned text data to a CSV file\n",
        "output_path = \"output.csv\"\n",
        "# Set index=False to exclude the index column in the output\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Text data cleaned and saved to:\", output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txDaR01oK2LQ",
        "outputId": "a574d581-dbb1-4da0-83ea-32ee0b61ab51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text data cleaned and saved to: output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.unique_words = self.get_unique_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index,\n",
        "                              word in enumerate(self.unique_words)}\n",
        "        self.word_to_index = {word: index for index,\n",
        "                              word in enumerate(self.unique_words)}\n",
        "\n",
        "        self.word_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        train_df = pd.read_csv('/content/output.csv')\n",
        "        text = train_df['Text'].str.cat(sep=' ')\n",
        "        return text.split(' ')\n",
        "\n",
        "    def get_unique_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_indexes) - self.args\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.word_indexes[index:index + self.args]),\n",
        "            torch.tensor(self.word_indexes[index + 1:index + self.args+ 1])\n",
        "        )"
      ],
      "metadata": {
        "id": "eq7TAYlyK5L0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "\n",
        "        n_vocab = len(dataset.unique_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (\n",
        "            torch.zeros(self.num_layers,\n",
        "                        sequence_length, self.lstm_size),\n",
        "            torch.zeros(self.num_layers,\n",
        "                        sequence_length, self.lstm_size)\n",
        "        )"
      ],
      "metadata": {
        "id": "6KaGWXenL0U-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import  DataLoader, random_split\n",
        "\n",
        "# Hyperparameters\n",
        "sequence_length = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Create the dataset\n",
        "dataset = TextDataset(sequence_length)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset,\n",
        "                                 [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                      batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = LSTMModel(dataset)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden = model.init_state(sequence_length)\n",
        "        outputs, _ = model(inputs, hidden)\n",
        "\n",
        "        loss = criterion(outputs.view(-1,\n",
        "                      len(dataset.unique_words)),\n",
        "                         targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Print the epoch and average loss\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "\n",
        "            hidden = model.init_state(sequence_length)\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "\n",
        "            loss = criterion(outputs.view(-1,\n",
        "                              len(dataset.unique_words)),\n",
        "                             targets.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Print the epoch and average validation loss\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgAJ4E2vMC5L",
        "outputId": "8ad5d197-3b92-4781-cb64-a5a823902637"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Average Loss: 6.5222\n",
            "Epoch [1/10], Validation Loss: 6.3566\n",
            "Epoch [2/10], Average Loss: 6.2262\n",
            "Epoch [2/10], Validation Loss: 6.0661\n",
            "Epoch [3/10], Average Loss: 5.9724\n",
            "Epoch [3/10], Validation Loss: 5.8813\n",
            "Epoch [4/10], Average Loss: 5.8223\n",
            "Epoch [4/10], Validation Loss: 5.7504\n",
            "Epoch [5/10], Average Loss: 5.7009\n",
            "Epoch [5/10], Validation Loss: 5.6381\n",
            "Epoch [6/10], Average Loss: 5.6047\n",
            "Epoch [6/10], Validation Loss: 5.5581\n",
            "Epoch [7/10], Average Loss: 5.5324\n",
            "Epoch [7/10], Validation Loss: 5.4946\n",
            "Epoch [8/10], Average Loss: 5.4693\n",
            "Epoch [8/10], Validation Loss: 5.4319\n",
            "Epoch [9/10], Average Loss: 5.4072\n",
            "Epoch [9/10], Validation Loss: 5.3682\n",
            "Epoch [10/10], Average Loss: 5.3442\n",
            "Epoch [10/10], Validation Loss: 5.3028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"he want to\"\n",
        "\n",
        "# Preprocess the input sentence\n",
        "input_indexes = [dataset.word_to_index[word] for word in input_sentence.split()]\n",
        "input_tensor = torch.tensor(input_indexes,dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "# Generate the next word\n",
        "model.eval()\n",
        "hidden = model.init_state(len(input_indexes))\n",
        "outputs, _ = model(input_tensor, hidden)\n",
        "predicted_index = torch.argmax(outputs[0, -1, :]).item()\n",
        "predicted_word = dataset.index_to_word[predicted_index]\n",
        "\n",
        "# Print the predicted word\n",
        "print(\"Input Sentence:\", input_sentence)\n",
        "print(\"Predicted Next Word:\", predicted_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cawA__EGMKKs",
        "outputId": "cc8b9803-b890-4897-8546-b4788780ee9d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: he want to\n",
            "Predicted Next Word: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLOncQ3cT9uP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}